{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Vector Quantised Variational Autoencoder </h1>\n",
    "Vector Quantized Variational Autoencoders (VQ-VAE) take a different approach to learning representations. While traditional VAEs enforce smoothness in the latent space with a continuous distribution, VQ-VAEs use a discrete latent space. Instead of learning continuous values, the encoder maps inputs to discrete \"codebook entries\". This quantization process helps capture meaningful structure and patterns in the data, making it possible to be used for generating high-quality samples, especially in domains like image or audio generation.\n",
    "<img src=\"../data/VQVAE.png\" width=\"1100\" align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yhWb2qkq6Idq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as Datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as vutils\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vyfSkLIu6Id3"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "lr = 1e-4\n",
    "\n",
    "root = \"../../datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ab2W41mB6Id6"
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "gpu_indx  = 0\n",
    "device = torch.device(gpu_indx if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create an MNIST dataset and dataloader</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6195,
     "status": "ok",
     "timestamp": 1570409783041,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "RJUrSrOl6Id-",
    "outputId": "a37cfcb0-da67-4107-fc85-893028c5d2cf"
   },
   "outputs": [],
   "source": [
    "# Define our transform\n",
    "# We'll upsample the images to 32x32 as it's easier to contruct our network\n",
    "transform = transforms.Compose([transforms.Resize(32),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize(32),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "train_set = Datasets.MNIST(root=root, train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size,shuffle=True, num_workers=4)\n",
    "\n",
    "test_set = Datasets.MNIST(root=root, train=False, transform=test_transform, download=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LgqQSvQg6IeG"
   },
   "source": [
    "## VQ-VAE Network\n",
    "The structure of a VQ-VAE is similar to a standard Autoencoder, but instead of producing continuous latent vectors, the encoder maps inputs to discrete latent codes from a fixed codebook. The encoder outputs are quantized by selecting the closest codebook vector for each input, which is then passed to the decoder for reconstruction.\n",
    "\n",
    "Unlike VAEs, VQ-VAEs don't sample from a distribution like N(μ,σ). Instead, the quantization step enforces a discrete, structured latent space. During training, the model uses a combination of reconstruction and commitment losses to ensure the encoder learns meaningful codebook representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, code_book_size, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.code_book_size = code_book_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "        self.embedding = nn.Embedding(code_book_size, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1/code_book_size, 1/code_book_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()  # BSxCxHxW --> BSxHxWxC\n",
    "        input_shape = inputs.shape\n",
    "        \n",
    "        flat_input = inputs.view(-1, 1, self.embedding_dim)  # BSxHxWxC --> BS*H*Wx1xC\n",
    "        \n",
    "        # Calculate the distance between each embedding and each codebook vector\n",
    "        distances = (flat_input - self.embedding.weight.unsqueeze(0)).pow(2).mean(2)  # BS*H*WxN\n",
    "        \n",
    "        # Find the closest codebook vector\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)  # BS*H*Wx1\n",
    "        \n",
    "        # Select that codebook vector\n",
    "        quantized = self.embedding(encoding_indices).view(input_shape)\n",
    "        \n",
    "        # Create loss that pulls encoder embeddings and codebook vector selected\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "        \n",
    "        # Reconstruct quantized representation using the encoder embeddings to allow for \n",
    "        # backpropagation of gradients into encoder\n",
    "        if self.training:\n",
    "            quantized = inputs + (quantized - inputs).detach()\n",
    "        \n",
    "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), encoding_indices.reshape(input_shape[0], -1)\n",
    "\n",
    "    \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.norm1 = nn.GroupNorm(8, channels)\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        self.norm2 = nn.GroupNorm(8, channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skip = x\n",
    "        \n",
    "        x = F.elu(self.norm1(x))\n",
    "        x = F.elu(self.norm2(self.conv1(x)))\n",
    "        x = self.conv2(x) + skip\n",
    "        return x\n",
    "\n",
    "\n",
    "# We split up our network into two parts, the Encoder and the Decoder\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super(DownBlock, self).__init__()\n",
    "        self.bn1 = nn.GroupNorm(8, channels_in)\n",
    "        self.conv1 = nn.Conv2d(channels_in, channels_out, 3, 2, 1)\n",
    "        self.bn2 = nn.GroupNorm(8, channels_out)\n",
    "        self.conv2 = nn.Conv2d(channels_out, channels_out, 3, 1, 1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(channels_in, channels_out, 3, 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.bn1(x))\n",
    "                  \n",
    "        x_skip = self.conv3(x)\n",
    "        \n",
    "        x = F.elu(self.bn2(self.conv1(x)))        \n",
    "        return self.conv2(x) + x_skip\n",
    "    \n",
    "    \n",
    "# We split up our network into two parts, the Encoder and the Decoder\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super(UpBlock, self).__init__()\n",
    "        self.bn1 = nn.GroupNorm(8, channels_in)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(channels_in, channels_in, 3, 1, 1)\n",
    "        self.bn2 = nn.GroupNorm(8, channels_in)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(channels_in, channels_out, 3, 1, 1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(channels_in, channels_out, 3, 1, 1)\n",
    "        self.up_nn = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        x = self.up_nn(F.elu(self.bn1(x_in)))\n",
    "        \n",
    "        x_skip = self.conv3(x)\n",
    "        \n",
    "        x = F.elu(self.bn2(self.conv1(x)))\n",
    "        return self.conv2(x) + x_skip\n",
    "\n",
    "    \n",
    "# We split up our network into two parts, the Encoder and the Decoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels, ch=32, latent_channels=32):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(channels, ch, 3, 1, 1)\n",
    "        \n",
    "        self.conv_block1 = DownBlock(ch, ch * 2)\n",
    "        self.conv_block2 = DownBlock(ch * 2, ch * 4)\n",
    "\n",
    "        # Instead of flattening (and then having to unflatten) out our feature map and \n",
    "        # putting it through a linear layer we can just use a conv layer\n",
    "        # where the kernal is the same size as the feature map \n",
    "        # (in practice it's the same thing)\n",
    "        self.res_block_1 = ResBlock(ch * 4)\n",
    "        self.res_block_2 = ResBlock(ch * 4)\n",
    "        self.res_block_3 = ResBlock(ch * 4)\n",
    "\n",
    "        self.conv_out = nn.Conv2d(4 * ch, latent_channels, 3, 1, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        \n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "\n",
    "        x = self.res_block_1(x)\n",
    "        x = self.res_block_2(x)\n",
    "        x = F.elu(self.res_block_3(x))\n",
    "\n",
    "        return self.conv_out(x)\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels, ch = 32, latent_channels = 32):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(latent_channels, 4 * ch, 3, 1, 1)\n",
    "        self.res_block_1 = ResBlock(ch * 4)\n",
    "        self.res_block_2 = ResBlock(ch * 4)\n",
    "        self.res_block_2 = ResBlock(ch * 4)\n",
    "\n",
    "        self.conv_block1 = UpBlock(4 * ch, 2 * ch)\n",
    "        self.conv_block2 = UpBlock(2 * ch, ch)\n",
    "        self.conv_out = nn.Conv2d(ch, channels, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.res_block_1(x)\n",
    "        x = self.res_block_2(x)\n",
    "        x = self.res_block_2(x)\n",
    "\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        \n",
    "        return torch.tanh(self.conv_out(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-TN2KYN6IeH"
   },
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, channel_in, ch=16, latent_channels=32, code_book_size=64, commitment_cost=0.25):\n",
    "        super(VQVAE, self).__init__()\n",
    "        self.encoder = Encoder(channels=channel_in, ch=ch, latent_channels=latent_channels)\n",
    "        \n",
    "        self.vq = VectorQuantizer(code_book_size=code_book_size, \n",
    "                                  embedding_dim=latent_channels, \n",
    "                                  commitment_cost=commitment_cost)\n",
    "        \n",
    "        self.decoder = Decoder(channels=channel_in, ch=ch, latent_channels=latent_channels)\n",
    "\n",
    "    def encode(self, x):\n",
    "        encoding = self.encoder(x)\n",
    "        vq_loss, quantized, encoding_indices = self.vq(encoding)\n",
    "        return vq_loss, quantized, encoding_indices\n",
    "        \n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        vq_loss, quantized, encoding_indices = self.encode(x)\n",
    "        recon = self.decode(quantized)\n",
    "        \n",
    "        return recon, vq_loss, quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Visualize our data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6489,
     "status": "ok",
     "timestamp": 1570409783350,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "74l6KlI06IeK",
    "outputId": "8d7a863d-8de8-4ae0-c4c1-403ddb67eae5"
   },
   "outputs": [],
   "source": [
    "# Get a test image\n",
    "dataiter = iter(test_loader)\n",
    "test_images = next(dataiter)[0]\n",
    "\n",
    "# View the shape\n",
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7133,
     "status": "ok",
     "timestamp": 1570409784001,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "GBE2TPmy6IeN",
    "outputId": "8344a523-4b6a-4adb-ecdd-c59fd617b137"
   },
   "outputs": [],
   "source": [
    "# Visualize the data!!!\n",
    "plt.figure(figsize = (5,5))\n",
    "out = vutils.make_grid(test_images, normalize=True)\n",
    "plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create Network and Optimizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XuZmm4Jx6IeQ"
   },
   "outputs": [],
   "source": [
    "# The number of code book embeddings\n",
    "code_book_size = 32\n",
    "\n",
    "# The number of latent embedding channels\n",
    "latent_channels = 10\n",
    "\n",
    "# Number of Training epochs\n",
    "vq_nepoch = 50\n",
    "\n",
    "# Create our network\n",
    "vae_net = VQVAE(channel_in=test_images.shape[1], latent_channels=latent_channels, ch=16, \n",
    "                code_book_size=code_book_size, commitment_cost=0.25).to(device)\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = optim.Adam(vae_net.parameters(), lr=lr)\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=vq_nepoch, eta_min=0)\n",
    "\n",
    "# Create loss logger\n",
    "recon_loss_log = []\n",
    "qv_loss_log = []\n",
    "test_recon_loss_log = []\n",
    "train_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in vae_net.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-The VQVAE Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, \n",
    "                                                                          num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Network output</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7126,
     "status": "ok",
     "timestamp": 1570409784003,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "iteROyrA6IeT",
    "outputId": "51104e6b-af60-44b9-a7f6-5885749d92af"
   },
   "outputs": [],
   "source": [
    "# Pass through a test image to make sure everything is working\n",
    "recon_data, vq_loss, quantized = vae_net(test_images.to(device))\n",
    "\n",
    "# View the Latent vector shape\n",
    "quantized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Start training!</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1570410601202,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "bVFA6W6L6IeY",
    "outputId": "99cd69e0-8ff6-466b-fb59-4e4ba9f33271"
   },
   "outputs": [],
   "source": [
    "pbar = trange(0, vq_nepoch, leave=False, desc=\"Epoch\")   \n",
    "for epoch in pbar:\n",
    "    pbar.set_postfix_str('Loss: %.4f' % (train_loss/len(train_loader)))\n",
    "    train_loss = 0\n",
    "    vae_net.train()\n",
    "    for i, data in enumerate(tqdm(train_loader, leave=False, desc=\"Training\")):\n",
    "\n",
    "        image = data[0].to(device)\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            # Forward pass the image in the data tuple\n",
    "            recon_data, vq_loss, quantized = vae_net(image)\n",
    "\n",
    "            # Calculate the loss\n",
    "            recon_loss = (recon_data - image).pow(2).mean()\n",
    "            loss = vq_loss + recon_loss\n",
    "\n",
    "        # Take a training step\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Log the loss\n",
    "        recon_loss_log.append(recon_loss.item())\n",
    "        qv_loss_log.append(vq_loss.item())\n",
    "        train_loss += recon_loss.item()\n",
    "        \n",
    "    lr_scheduler.step()\n",
    "\n",
    "    vae_net.eval()\n",
    "    for i, data in enumerate(tqdm(test_loader, leave=False, desc=\"Testing\")):\n",
    "        image = data[0].to(device)\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            with torch.no_grad():\n",
    "                # Forward pass the image in the data tuple\n",
    "                recon_data, vq_loss, quantized = vae_net(image)\n",
    "\n",
    "                # Calculate the loss\n",
    "                recon_loss = (recon_data - image).pow(2).mean()\n",
    "                loss = vq_loss + recon_loss\n",
    "                test_recon_loss_log.append(recon_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0ZrSDsR6Ief"
   },
   "source": [
    "## Results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 824609,
     "status": "ok",
     "timestamp": 1570410601500,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "vTr64GEm6Iej",
    "outputId": "4d715e03-42e0-4277-ec5a-1e5577c2b240"
   },
   "outputs": [],
   "source": [
    "x_train = np.linspace(0, vq_nepoch, len(recon_loss_log[200:]))\n",
    "_ = plt.plot(x_train, recon_loss_log[200:])\n",
    "\n",
    "x_test = np.linspace(0, vq_nepoch, len(test_recon_loss_log[200:]))\n",
    "_ = plt.plot(x_test, test_recon_loss_log[200:])\n",
    "_ = plt.title(\"Reconstruction Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(qv_loss_log[100:])\n",
    "_ = plt.title(\"VQ Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_net.eval()\n",
    "recon_data, vq_loss, quantized = vae_net(test_images.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_loss, quantized, encoding_indices = vae_net.encode(test_images.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_indices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7GW3ruNa6Ieo"
   },
   "source": [
    "Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 825552,
     "status": "ok",
     "timestamp": 1570410602450,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "yqC3cmVx6Ieo",
    "outputId": "8665a80d-bc7d-4be4-d8fa-89716e7391c7"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(test_images[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FPRXvoNr6Ies"
   },
   "source": [
    "Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 825546,
     "status": "ok",
     "timestamp": 1570410602451,
     "user": {
      "displayName": "Luke Ditria",
      "photoUrl": "",
      "userId": "06313774588804829868"
     },
     "user_tz": -660
    },
    "id": "kX95LD-u6Iet",
    "outputId": "40c30d2b-837b-4143-a540-024b14789389"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "out = vutils.make_grid(recon_data.detach().cpu()[0:8], normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Sequence Generation with Transformers\n",
    "Once images are represented as a discrete sequence of embeddings, we can use a decoder-only Transformer to generate new sequences by predicting the next embedding in the sequence. Similar to text generation models, this approach leverages the Transformer’s ability to model long-range dependencies. By applying the chain rule of probabilities, the model generates complex distributions, one embedding at a time, building up the image sequentially. This method allows for high-quality image generation by capturing intricate relationships between embeddings, much like how text models predict the next word in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "# Define an Encoder module for the Transformer architecture\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Create multiple transformer blocks as layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads,\n",
    "                                                   dim_feedforward=hidden_size * 4, dropout=0.0,\n",
    "                                                   batch_first=True)\n",
    "        # TransformerEncoder will clone the encoder_layer \"num_layers\" times\n",
    "        self.encoder_layers = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "    def forward(self, input_seq, padding_mask=None):\n",
    "        \n",
    "        bs, l, h = input_seq.shape\n",
    "        # Create the causal mask\n",
    "        causal_mask = torch.triu(torch.ones(l, l, device=input_seq.device), 1).bool()\n",
    "\n",
    "        # Pass the embeddings through each transformer block\n",
    "        output = self.encoder_layers(src=input_seq, mask=causal_mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # Create an embedding layer for tokens\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "\n",
    "        # Initialize sinusoidal positional embeddings\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "\n",
    "        # Create an encoder and decoder with specified parameters\n",
    "        self.encoder = Encoder(hidden_size=hidden_size, num_layers=num_layers,\n",
    "                               num_heads=num_heads)\n",
    "\n",
    "        # Define a linear layer for output prediction\n",
    "        self.fc_out = nn.Linear(hidden_size, num_emb)\n",
    "\n",
    "    def embed(self, input_seq):\n",
    "        # Embed the input sequence\n",
    "        input_embs = self.embedding(input_seq)\n",
    "        bs, l, h = input_embs.shape\n",
    "\n",
    "        # Add positional embeddings to the input embeddings\n",
    "        seq_indx = torch.arange(l, device=input_seq.device)\n",
    "        pos_emb = self.pos_emb(seq_indx).reshape(1, l, h).expand(bs, l, h)\n",
    "        embs = input_embs + pos_emb\n",
    "        return embs\n",
    "\n",
    "    def encode(self, input_seq):\n",
    "        # Embed the input sequence\n",
    "        embs = self.embed(input_seq)\n",
    "\n",
    "        # Encode the sequence\n",
    "        embs_out = self.encoder(embs)\n",
    "        return embs_out\n",
    "\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # Encode the input sequence\n",
    "        encoded_seq = self.encode(input_seq=input_seq)\n",
    "\n",
    "        return self.fc_out(encoded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of transformer blocks\n",
    "num_layers = 4\n",
    "\n",
    "# MultiheadAttention Heads\n",
    "num_heads = 8\n",
    "\n",
    "hidden_size = 256\n",
    "\n",
    "# Number of Training epochs\n",
    "tf_nepoch = 100\n",
    "\n",
    "# Create model\n",
    "# We'll include a \"start-sequence\" token so there are num_embeddings + 1 embeddings\n",
    "tf_generator = Transformer(num_emb=code_book_size + 1, num_layers=num_layers, \n",
    "                           hidden_size=hidden_size, num_heads=num_heads).to(device)\n",
    "\n",
    "# Initialize the optimizer with above parameters\n",
    "tf_optimizer = optim.Adam(tf_generator.parameters(), lr=lr)\n",
    "\n",
    "tf_lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(tf_optimizer, T_max=tf_nepoch, eta_min=0)\n",
    "\n",
    "# Scaler for mixed precision training\n",
    "tf_scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize training loss logger\n",
    "training_loss_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many Parameters our Model has!\n",
    "num_model_params = 0\n",
    "for param in tf_generator.parameters():\n",
    "    num_model_params += param.flatten().shape[0]\n",
    "\n",
    "print(\"-The TF Model Has %d (Approximately %d Million) Parameters!\" % (num_model_params, num_model_params//1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = trange(0, tf_nepoch, leave=False, desc=\"Epoch\")   \n",
    "vae_net.eval()\n",
    "for epoch in pbar:\n",
    "    pbar.set_postfix_str('Loss: %.4f' % (train_loss/len(train_loader)))\n",
    "    train_loss = 0\n",
    "    \n",
    "    tf_generator.train()\n",
    "    for i, data in enumerate(tqdm(train_loader, leave=False, desc=\"Training\")):\n",
    "        image = data[0].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, _, encoding_indices = vae_net.encode(image)\n",
    "        \n",
    "        encoding_indices = encoding_indices + 1\n",
    "        tf_inputs = torch.cat((torch.zeros_like(encoding_indices[:, 0:1]), encoding_indices[:, :-1]), 1)\n",
    "        tf_outputs = encoding_indices\n",
    "\n",
    "        # Generate predictions\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            pred = tf_generator(tf_inputs)\n",
    "\n",
    "        loss = loss_fn(pred.transpose(1, 2), tf_outputs)\n",
    "        \n",
    "        # Backpropagation\n",
    "        tf_optimizer.zero_grad()\n",
    "        tf_scaler.scale(loss).backward()\n",
    "        tf_scaler.step(tf_optimizer)\n",
    "        tf_scaler.update()\n",
    "\n",
    "        # Log training loss and entropy\n",
    "        training_loss_logger.append(loss.item())\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    tf_lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(training_loss_logger[200:])\n",
    "_ = plt.title(\"Loss per iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set temperature for sampling\n",
    "temp = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to log generated tokens\n",
    "log_tokens = [torch.zeros(64, 1).long()]\n",
    "\n",
    "# Set the generator model to evaluation mode\n",
    "tf_generator.eval()\n",
    "\n",
    "# Generate tokens\n",
    "with torch.no_grad():    \n",
    "    for i in range(64):\n",
    "        # Concatenate tokens from previous iterations\n",
    "        input_tokens = torch.cat(log_tokens, 1)\n",
    "        \n",
    "        # Get model predictions for the next token\n",
    "        data_pred = tf_generator(input_tokens.to(device))\n",
    "        \n",
    "        # Sample the next token from the distribution of probabilities\n",
    "        dist = Categorical(logits=data_pred[:, -1] / temp)\n",
    "        next_tokens = dist.sample().reshape(-1, 1)\n",
    "        \n",
    "        # Append the sampled token to the list of generated tokens\n",
    "        log_tokens.append(next_tokens.cpu())\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(F.softmax(data_pred[0, -1], -1).flatten().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_indx = torch.cat(log_tokens, 1)[:, 1:].to(device) - 1\n",
    "embeds = vae_net.vq.embedding(embs_indx).reshape(-1, 8, 8, latent_channels).permute(0, 3, 1, 2).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_data = vae_net.decode(embeds)\n",
    "\n",
    "plt.figure(figsize = (5,5))\n",
    "out = vutils.make_grid(recon_data.detach().cpu(), normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_sample = torch.randint(code_book_size, (64, 64), device=device)\n",
    "rand_sample_embeds = vae_net.vq.embedding(rand_sample).reshape(-1, 8, 8, latent_channels).permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "recon_data = vae_net.decode(rand_sample_embeds)\n",
    "\n",
    "plt.figure(figsize = (5,5))\n",
    "out = vutils.make_grid(recon_data.detach().cpu(), normalize=True)\n",
    "_ = plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "VAE.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
